{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.decomposition\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation, SpectralClustering, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h2>Notebook details</h2>\n",
    "\n",
    "<p> This notebook is for <b>Clustering</b> for <b>Mortagage custome segementation</b> project.</p>\n",
    "<p> The records are choosen by test and train. The loan amount is considerd as one feature. The salary and loan are used as amount</p>\n",
    "<p> Notes.</p>\n",
    "<ol>\n",
    "<li>Apply K Mean clustering algorithm to the data</li>\n",
    "<li>Apply methods to choose best value of K</li> \n",
    "    <ul>\n",
    "     <li>The Elbow Sum-of-Squares Method</li>\n",
    "     <li>The Silhouette Method </li>\n",
    "    </ul>\n",
    "<li>Consider K Mean as baseline analysis </li>\n",
    "<li>Apply test train split to reduce data size to one million records</li>\n",
    "        \n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the size of data set and allow the code to analyze cluster in each state the data frame filtered by State\n",
    "def getDataFrameforState(inputframe,stateName='CA'):\n",
    "    df=inputframe[inputframe.StateName==stateName]\n",
    "    df=df[df.Accepted>0]\n",
    "    #df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get algo for no of cluster \n",
    "def getAlgoForCluster(algoType, noOfCluster, randomstate=10):\n",
    "    if(algoType.lower()=='kmean'):\n",
    "        kmean = KMeans(n_clusters=noOfCluster, random_state=randomstate)\n",
    "    elif(algoType.lower()=='spectcluster'):\n",
    "        return SpectralClustering(n_clusters=noOfCluster)\n",
    "    elif(algoType.lower()=='affipropogation'):\n",
    "        return  AffinityPropagation(damping=noOfCluster)\n",
    "    elif(algoType.lower()=='algocluster'):\n",
    "        return AgglomerativeClustering(n_clusters=noOfCluster)\n",
    "    elif(algoType.lower()=='dbscan'):\n",
    "        return DBSCAN(min_samples=noOfCluster)\n",
    "    else :\n",
    "        return None\n",
    "        \n",
    "    return kmean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fit predcit for Algo\n",
    "def getFitPredictForAlgo(kMean,xcols):\n",
    "    algo_val =kMean.fit_predict(xcols)\n",
    "    return algo_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data frame  with PCA with component dimension and K mean for \n",
    "#one component as all x cols and another component as the unique value\n",
    "# this provides the clutser for each row in data frame \n",
    "def getPCADataFrame(df,noOfCluster,algo_val,xcols,componetNum=2):\n",
    "    pca = PCA(n_components = componetNum)\n",
    "    matrix = np.matrix(pca.fit_transform(xcols))\n",
    "    df_pca_matrix = pd.DataFrame(matrix)\n",
    "    df_pca_matrix.columns = ['x','y']\n",
    "\n",
    "    df_clusters = pd.DataFrame(df.iloc[:,0])\n",
    "    #df_clusters['x'], df_clusters['y'] = df_pca_matrix['x'], df_pca_matrix['y']\n",
    "    #df_clusters['cluster_label'] = algo_val\n",
    "    #df_clusters['x']  = np.NAN\n",
    "    df_clusters['x']  = df_pca_matrix['x'].values\n",
    "    #df_clusters['y']  = np.NAN\n",
    "    df_clusters['y']  = df_pca_matrix['y'].values\n",
    "    df_clusters['cluster_label'] = algo_val\n",
    "\n",
    "    return df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to draw average silhouette score as graph for ranges of clusterand avg score calculated\n",
    "def drawAverageSilhouetteScore(range_n_clusters,silhouette_avgscores,algo_Name):\n",
    "        fig, axis = plt.subplots(1,1,figsize=(6,6),dpi=100)\n",
    "        _ = plt.plot(range_n_clusters,silhouette_avgscores)\n",
    "        _ = plt.xlabel('$K$')\n",
    "        _ = plt.ylabel('Average Silhouette Score')\n",
    "        _ = plt.title('Average Silhouette Scores for '+algo_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw bar graph for number of rows for each cluster X axis(cluster number), Y axis(no of rows in each cluster)\n",
    "def drawClusterBar(noOfCluster,algo_val):\n",
    "    cluster_algo_val = pd.Series(algo_val).value_counts().sort_index()\n",
    "    #print(cluster_algo_val5)\n",
    "\n",
    "    fig, axis = plt.subplots(1,1,figsize=(6,6),dpi=100)\n",
    "    _ = cluster_algo_val.plot(kind='bar')\n",
    "    _ = plt.ylabel('Number of Points')\n",
    "    _ = plt.xlabel('Cluster')\n",
    "    _ = plt.title('No of points for Clusters($K$ = '+str(noOfCluster)+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw cluster point on graph for each cluster . depends on data frame created PCA\n",
    "def drawClusterPlot(df_clusters):\n",
    "    axis = sns.lmplot(data=df_clusters, x='x', y='y', hue='cluster_label', \n",
    "                   fit_reg=False, legend=True, legend_out=True,size=10)\n",
    "    _ = axis.set_axis_labels(\"Component 1\", \"Component 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw elbow plot to get the best component value for PCA\n",
    "def drawElbowPCAplot(xcols):\n",
    "    pca = sklearn.decomposition.PCA()\n",
    "    pca.fit(xcols)\n",
    "    fig, axis = plt.subplots(1,1,figsize=(12,6),dpi=100)\n",
    "    _ = plt.plot(pca.explained_variance_)\n",
    "    _ = plt.xlabel('$K$')\n",
    "    _ = plt.xticks(range(0,33,1))\n",
    "    _ = plt.xlim([0,31])\n",
    "    _ = plt.ylabel('Explained Variance')\n",
    "    _ = plt.title('Elbow Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllcolNameforDataframe(df):\n",
    "    for col in df:\n",
    "        print(\"'\"+str(col)+\"',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllColSum(df):\n",
    "    for col in df:\n",
    "        col_sum=sum(df[col])\n",
    "        col_n=len(df[col])\n",
    "        print('Sum of col : '+ col +' is '+str(col_sum) + ' total of col is ' +str(col_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareColForDataFrame(df1,df2):\n",
    "    for col in df1:\n",
    "        col_sum1=sum(df1[col])\n",
    "        #col_n1=len(df1[col])\n",
    "        #print('Sum of col : '+ col +' is '+str(col_sum) + ' total of col is ' +str(col_n))\n",
    "        col_sum2=sum(df2[col])\n",
    "        #col_n2=len(df2[col])\n",
    "        diff =col_sum1-col_sum2\n",
    "        tot=col_sum1+col_sum2\n",
    "        print('Sum of col  : '+ col +' is '+str(col_sum1) + ' vs ' +str(col_sum2) +' diff ='+str(diff)+ ' ratio col1 :'+str((col_sum1/tot)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterDatasetForRowCount(df,noofRows,random=True, samplesize=0.5):\n",
    "    if(len(df)>noofRows):\n",
    "        if(random):\n",
    "            df1,df2=train_test_split(df, shuffle=True,train_size=samplesize,test_size=samplesize)\n",
    "            if(len(df1)>noofRows):\n",
    "                df=df1.iloc[:noofRows,:]\n",
    "            else:\n",
    "                 df=(df1.append(df2,ignore_index=True)).iloc[:noofRows,:]\n",
    "        else:\n",
    "            df=df.iloc[:noofRows,:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to convert the category column into dummy columns \n",
    "def AddDummyColumnsToDataFrame(dfinput,colname,removeOrgColumn=False,removelastdummy=False):\n",
    "    print('Add {}'.format(colname))\n",
    "    temp =pd.get_dummies(dfinput[colname])\n",
    "    # remove one column from dummies with least value.\n",
    "  \n",
    "    if removelastdummy:\n",
    "        t=dfinput.groupby(colname).count().state\n",
    "        col_name=((t[t.values==t.min()]).index).get_values()[0]\n",
    "        if col_name in temp.columns:\n",
    "            print('removed column {}'.format(col_name))\n",
    "            temp=temp.drop([col_name], axis=1)\n",
    "    \n",
    "    # remove the main column after extracting dummy\n",
    "    if removeOrgColumn:\n",
    "        if colname in dfinput.columns:\n",
    "            print('removed column {}'.format(colname))\n",
    "            dfinput =dfinput.drop([colname], axis=1)\n",
    "    else:\n",
    "        print('left column {} in dataframe'.format(colname))\n",
    "        \n",
    "        \n",
    "    for col in temp:\n",
    "        temp.rename(columns={col: colname+'_'+str(col)}, inplace=True)\n",
    "    \n",
    "    return  pd.concat([dfinput,temp], axis=1,ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file='pickle_selectdata_ML_All_Col_CA.sa'\n",
    "df_filterdata = pickle.load( open( pickle_file, \"rb\" ) )\n",
    "#df_final_months.info()\n",
    "#df_final_months.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3179129"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_selectdata.info()\n",
    "len(df_filterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Year</th>\n",
       "      <th>PropertyType</th>\n",
       "      <th>LoanPurpose</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>ActionType</th>\n",
       "      <th>MSA</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>CountyCode</th>\n",
       "      <th>...</th>\n",
       "      <th>LonAmt_100_150</th>\n",
       "      <th>LonAmt_150_200</th>\n",
       "      <th>LonAmt_200_250</th>\n",
       "      <th>LonAmt_250_300</th>\n",
       "      <th>LonAmt_300_350</th>\n",
       "      <th>LonAmt_350_400</th>\n",
       "      <th>LonAmt_400_450</th>\n",
       "      <th>LonAmt_450_500</th>\n",
       "      <th>LonAmt_500_5500</th>\n",
       "      <th>LonAmt_5500_999999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280124</th>\n",
       "      <td>280416</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280125</th>\n",
       "      <td>280417</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280126</th>\n",
       "      <td>280418</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280127</th>\n",
       "      <td>280419</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280128</th>\n",
       "      <td>280420</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Year  PropertyType  LoanPurpose  Occupancy  LoanAmount  \\\n",
       "280124  280416  2015             1          3.0        2.0       380.0   \n",
       "280125  280417  2015             1          1.0        1.0       404.0   \n",
       "280126  280418  2015             1          3.0        1.0       415.0   \n",
       "280127  280419  2015             1          3.0        1.0       325.0   \n",
       "280128  280420  2015             1          3.0        1.0       322.0   \n",
       "\n",
       "        ActionType      MSA  StateCode  CountyCode        ...          \\\n",
       "280124         1.0  11244.0        6.0        59.0        ...           \n",
       "280125         1.0  11244.0        6.0        59.0        ...           \n",
       "280126         1.0  11244.0        6.0        59.0        ...           \n",
       "280127         1.0  11244.0        6.0        59.0        ...           \n",
       "280128         1.0  11244.0        6.0        59.0        ...           \n",
       "\n",
       "        LonAmt_100_150  LonAmt_150_200  LonAmt_200_250  LonAmt_250_300  \\\n",
       "280124               0               0               0               0   \n",
       "280125               0               0               0               0   \n",
       "280126               0               0               0               0   \n",
       "280127               0               0               0               0   \n",
       "280128               0               0               0               0   \n",
       "\n",
       "        LonAmt_300_350  LonAmt_350_400  LonAmt_400_450  LonAmt_450_500  \\\n",
       "280124               0               1               0               0   \n",
       "280125               0               0               1               0   \n",
       "280126               0               0               1               0   \n",
       "280127               1               0               0               0   \n",
       "280128               1               0               0               0   \n",
       "\n",
       "       LonAmt_500_5500 LonAmt_5500_999999  \n",
       "280124               0                  0  \n",
       "280125               0                  0  \n",
       "280126               0                  0  \n",
       "280127               0                  0  \n",
       "280128               0                  0  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filterdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 million rows extracted for state to process\n",
    "noofRows=100#000\n",
    "range_n_clusters = range(2,11)\n",
    "df_filterdata=getFilterDatasetForRowCount(getDataFrameforState(df_filterdata,'CA'),noofRows)\n",
    "#df_filterdata_NR=getFilterDatasetForRowCount(getDataFrameforState(df_selectdata,'CA'),noofRows,False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_=df_filterdata.reset_index(inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filterdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountyCode\n",
      "Add CountyCode\n",
      "left column CountyCode in dataframe\n"
     ]
    }
   ],
   "source": [
    "# Convert category columns to dummy columns=\n",
    "categoryColumns=['CountyCode']\n",
    "for col in categoryColumns:\n",
    "    print(col)\n",
    "    df_filterdata=AddDummyColumnsToDataFrame(df_filterdata,col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_filterdata.iloc[:,60:85].head()\n",
    "#trying with excluding males\n",
    "#len(df_filterdata[df_filterdata['ApplicantSex_1.0']!=1])\n",
    "df_filterdata_male=df_filterdata[df_filterdata['ApplicantSex_1.0']==1]\n",
    "df_filterdata_Notmale=df_filterdata[df_filterdata['ApplicantSex_1.0']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create common x for all further processing for all feature columns \n",
    "x_cols = np.matrix(df_filterdata.iloc[:,69:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(df_filterdata))\n",
    "print (len(df_filterdata.index.unique()))\n",
    "print(len(x_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Algorithms\n",
    "\n",
    "k-means is only one of a ton of clustering algorithms. Below is a brief description of several clustering algorithms, and the table provides references to the other clustering algorithms in scikit-learn. \n",
    "\n",
    "* **Affinity Propagation** does not require the number of clusters $K$ to be known in advance! AP uses a \"message passing\" paradigm to cluster points based on their similarity. \n",
    "\n",
    "* **Spectral Clustering** uses the eigenvalues of a similarity matrix to reduce the dimensionality of the data before clustering in a lower dimensional space. This is tangentially similar to what we did to visualize k-means clusters using PCA. The number of clusters must be known a priori.\n",
    "\n",
    "* **Ward's Method** applies to hierarchical clustering. Hierarchical clustering algorithms take a set of data and successively divide the observations into more and more clusters at each layer of the hierarchy. Ward's method is used to determine when two clusters in the hierarchy should be combined into one. It is basically an extension of hierarchical clustering. Hierarchical clustering is *divisive*, that is, all observations are part of the same cluster at first, and at each successive iteration, the clusters are made smaller and smaller. With hierarchical clustering, a hierarchy is constructed, and there is not really the concept of \"number of clusters.\" The number of clusters simply determines how low or how high in the hierarchy we reference and can be determined empirically or by looking at the [dendogram](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.dendrogram.html).\n",
    "\n",
    "* **Agglomerative Clustering** is similar to hierarchical clustering but but is not divisive, it is *agglomerative*. That is, every observation is placed into its own cluster and at each iteration or level or the hierarchy, observations are merged into fewer and fewer clusters until convergence. Similar to hierarchical clustering, the constructed hierarchy contains all possible numbers of clusters and it is up to the analyst to pick the number by reviewing statistics or the dendogram.\n",
    "\n",
    "* **DBSCAN** is based on point density rather than distance. It groups together points with many nearby neighbors. DBSCAN is one of the most cited algorithms in the literature. It does not require knowing the number of clusters a priori, but does require specifying the neighborhood size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithms in Scikit-learn\n",
    "<table border=\"1\">\n",
    "<colgroup>\n",
    "<col width=\"15%\" />\n",
    "<col width=\"16%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"27%\" />\n",
    "<col width=\"22%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th>Method name</th>\n",
    "<th>Parameters</th>\n",
    "<th>Scalability</th>\n",
    "<th>Use Case</th>\n",
    "<th>Geometry (metric used)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td>K-Means</span></a></td>\n",
    "<td>number of clusters</td>\n",
    "<td>Very large<span class=\"pre\">n_samples</span>, medium <span class=\"pre\">n_clusters</span> with\n",
    "MiniBatch code</td>\n",
    "<td>General-purpose, even cluster size, flat geometry, not too many clusters</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Affinity propagation</td>\n",
    "<td>damping, sample preference</td>\n",
    "<td>Not scalable with n_samples</td>\n",
    "<td>Many clusters, uneven cluster size, non-flat geometry</td>\n",
    "<td>Graph distance (e.g. nearest-neighbor graph)</td>\n",
    "</tr>\n",
    "<tr><td>Mean-shift</td>\n",
    "<td>bandwidth</td>\n",
    "<td>Not scalable with <span class=\"pre\">n_samples</span></td>\n",
    "<td>Many clusters, uneven cluster size, non-flat geometry</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Spectral clustering</td>\n",
    "<td>number of clusters</td>\n",
    "<td>Medium <span class=\"pre\">n_samples</span>, small <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Few clusters, even cluster size, non-flat geometry</td>\n",
    "<td>Graph distance (e.g. nearest-neighbor graph)</td>\n",
    "</tr>\n",
    "<tr><td>Ward hierarchical clustering</td>\n",
    "<td>number of clusters</td>\n",
    "<td>Large <span class=\"pre\">n_samples</span> and <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Many clusters, possibly connectivity constraints</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Agglomerative clustering</td>\n",
    "<td>number of clusters, linkage type, distance</td>\n",
    "<td>Large <span class=\"pre\">n_samples</span> and <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Many clusters, possibly connectivity constraints, non Euclidean\n",
    "distances</td>\n",
    "<td>Any pairwise distance</td>\n",
    "</tr>\n",
    "<tr><td>DBSCAN</td>\n",
    "<td>neighborhood size</td>\n",
    "<td>Very large <span class=\"pre\">n_samples</span>, medium <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Non-flat geometry, uneven cluster sizes</td>\n",
    "<td>Distances between nearest points</td>\n",
    "</tr>\n",
    "<tr><td>Gaussian mixtures</td>\n",
    "<td>many</td>\n",
    "<td>Not scalable</td>\n",
    "<td>Flat geometry, good for density estimation</td>\n",
    "<td>Mahalanobis distances to  centers</td>\n",
    "</tr>\n",
    "<tr><td>Birch</td>\n",
    "<td>branching factor, threshold, optional global clusterer.</td>\n",
    "<td>Large <span class=\"pre\">n_clusters</span> and <span class=\"pre\">n_samples</span></td>\n",
    "<td>Large dataset, outlier removal, data reduction.</td>\n",
    "<td>Euclidean distance between points</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "Source: http://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Method for K \n",
    "<pre>\n",
    "\n",
    "| Range       | Interpretation                                |\n",
    "|-------------|-----------------------------------------------|\n",
    "| 0.71 - 1.0  | A strong structure has been found.            |\n",
    "| 0.51 - 0.7  | A reasonable structure has been found.        |\n",
    "| 0.26 - 0.5  | The structure is weak and could be artificial.|\n",
    "| < 0.25      | No substantial structure has been found.      |\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Affinity Propagation\n",
    "print(str(datetime.datetime.now()))\n",
    "#find the best value for damping parameter. \n",
    "dampings = [ 0.7, 0.8, 0.9,1.0,1.1]\n",
    "best_score = 0.0\n",
    "for damping in dampings:\n",
    "    ap = getAlgoForCluster('affipropogation', damping)#AffinityPropagation(damping=damping)\n",
    "    #labels = ap.fit_predict(x_cols)\n",
    "    labels = getFitPredictForAlgo(ap,x_cols)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(x_cols, labels, random_state=10)\n",
    "    print(\"For damping =\", damping,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_damping = damping\n",
    "        \n",
    "print('Best damping parameter is', best_damping)\n",
    "print ('Best AffinityPropagation score is', best_score)\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Spectral Clustering\n",
    "\n",
    "#find the best value for n_clusters parameter. \n",
    "print(str(datetime.datetime.now()))\n",
    "best_score = 0.0\n",
    "for n_clusters in range_n_clusters:\n",
    "    sc = getAlgoForCluster('spectcluster', n_clusters)#SpectralClustering(n_clusters=n_clusters)\n",
    "    #labels = sc.fit_predict(x_cols)\n",
    "    labels = getFitPredictForAlgo(sc,x_cols)\n",
    "    silhouette_avg = silhouette_score(x_cols, labels, random_state=10)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_n_clusters = n_clusters\n",
    "        \n",
    "print('Best n_clusters parameter:',best_n_clusters)\n",
    "print ('Best SpectralClustering score is', best_score)\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Agglomerative Clustering\n",
    "print(str(datetime.datetime.now()))\n",
    "#find the best value for n_clusters parameter. \n",
    "aag_predict_col={}\n",
    "aag_silh_score={}\n",
    "best_score = 0.0\n",
    "for n_clusters in range_n_clusters:\n",
    "    ac = getAlgoForCluster('algocluster', n_clusters)#AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    #labels = ac.fit_predict(x_cols)\n",
    "    labels = getFitPredictForAlgo(ac,x_cols)\n",
    "    if n_clusters not in aag_predict_col:\n",
    "        aag_predict_col[n_clusters]=labels\n",
    "    silhouette_avg = silhouette_score(x_cols, labels, random_state=10)\n",
    "    if n_clusters not in aag_silh_score:\n",
    "        aag_silh_score[n_clusters]=silhouette_avg\n",
    "        \n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_n_clusters = n_clusters\n",
    "        \n",
    "print('Best n_clusters parameter:',best_n_clusters)\n",
    "print ('Best AgglomerativeClustering score is', best_score)\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #range_n_clusters\n",
    "# #aag_silh_score.values()\n",
    "# # pca = PCA(n_components = 2)\n",
    "# # t=np.matrix(pca.fit_transform(x_cols))\n",
    "# # t\n",
    "# dftPCA=getPCADataFrame(df_filterdata,noofc2luster,algo_val,x_cols)\n",
    "# dftPCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# draw graph for all above silhouette_score\n",
    "drawAverageSilhouetteScore(range_n_clusters,aag_silh_score.values(),'Agglomerative Clustering' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize cluster for range"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for noofcluster in range_n_clusters:#[5,6,8,9,11]:#:range_n_clusters:\n",
    "    if noofcluster in aag_predict_col:\n",
    "        algo_val=aag_predict_col[noofcluster]\n",
    "    else:\n",
    "        algo_val = getFitPredictForAlgo(getAlgoForCluster('algocluster', noofcluster),x_cols)\n",
    "    drawClusterBar(noofcluster,algo_val)\n",
    "    df_PCA=getPCADataFrame(df_filterdata,noofcluster,algo_val,x_cols)\n",
    "    drawClusterPlot(df_PCA)\n",
    "    df_merge_w_cluster=pd.merge(df_filterdata, df_PCA,\n",
    "                                left_on = 'index', right_on = 'index',how='inner')\n",
    "    # save file for all clusters\n",
    "    picklefilename='pickle_AggClu_All_Col_CA_Cluster_'+str(noofcluster)+'.sa'\n",
    "    print(picklefilename)\n",
    "    # create pickle file for further use \n",
    "    pickle.dump(df_merge_w_cluster,open(picklefilename,'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-13 18:35:48.973103\n",
      "For min_samples = 1 The average silhouette_score is : 0.02\n",
      "For min_samples = 2 The average silhouette_score is : -0.06118004572025914\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-26b8158ed213>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmin_samples\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdbscan_predict_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdbscan_predict_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0msilhouette_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmin_samples\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdbscan_silh_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdbscan_silh_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msilhouette_avg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mcheck_number_of_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36mcheck_number_of_labels\u001b[1;34m(n_labels, n_samples)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         raise ValueError(\"Number of labels is %d. Valid values are 2 \"\n\u001b[1;32m---> 19\u001b[1;33m                          \"to n_samples - 1 (inclusive)\" % n_labels)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "#DBSCAN\n",
    "print(str(datetime.datetime.now()))\n",
    "#find the best value for min_samples parameter. \n",
    "range_min_samples = [1, 2, 3]\n",
    "dbscan_predict_col={}\n",
    "dbscan_silh_score={}\n",
    "best_score = 0.0\n",
    "for min_samples in range_min_samples:\n",
    "    db =  getAlgoForCluster('dbscan', min_samples)#DBSCAN(min_samples=min_samples)\n",
    "    #labels = db.fit_predict(x_cols)\n",
    "    labels = getFitPredictForAlgo(db,x_cols)\n",
    "    if min_samples not in dbscan_predict_col:\n",
    "        dbscan_predict_col[min_samples]=labels\n",
    "    silhouette_avg = silhouette_score(x_cols, labels, random_state=10)\n",
    "    if min_samples not in dbscan_silh_score:\n",
    "        dbscan_silh_score[min_samples]=silhouette_avg\n",
    "            \n",
    "    print(\"For min_samples =\", min_samples,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_min_samples = min_samples\n",
    "        \n",
    "print('Best min_samples parameter:',best_min_samples)\n",
    "print ('Best DBSCAN score is', best_score)\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw graph for all above silhouette_score\n",
    "drawAverageSilhouetteScore(range_min_samples,dbscan_silh_score.values(),'DB SCAN' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noofcluster in range_min_samples:\n",
    "    if noofcluster in dbscan_predict_col:\n",
    "        algo_val=dbscan_predict_col[noofcluster]\n",
    "    else:\n",
    "        algo_val = getFitPredictForAlgo(getAlgoForCluster('dbscan', min_samples),x_cols)\n",
    "    drawClusterBar(min_samples,algo_val)\n",
    "    df_PCA=getPCADataFrame(df_filterdata,min_samples,algo_val,x_cols)\n",
    "    drawClusterPlot(df_PCA)\n",
    "    df_merge_w_cluster=pd.merge(df_filterdata, df_PCA,\n",
    "                                left_on = 'index', right_on = 'index',how='inner')\n",
    "    # save file for all clusters\n",
    "    picklefilename='pickle_Dbscan_All_Col_CA_Cluster_'+str(noofcluster)+'.sa'\n",
    "    print(picklefilename)\n",
    "    # create pickle file for further use \n",
    "    pickle.dump(df_merge_w_cluster,open(picklefilename,'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### old below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_filterdata_male))\n",
    "print(len(df_filterdata_Notmale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_filterdata))\n",
    "print (len(df_filterdata.index.unique()))\n",
    "print(len(x_cols))\n",
    "\n",
    "#range_n_clusters = range(2,14)\n",
    "# print(len(df_filterdata_NR))\n",
    "# print (len(df_filterdata_NR.index.unique()))\n",
    "# print(len(x_cols_NR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K - Mean clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method for K \n",
    "We can plot SS vs. $K$ and choose the *elbow point* in the plot as the best value for $K$. The elbow point is the point at which the plot starts descending much more slowly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the K means for range to find out best K value for elbow method\n",
    "print(str(datetime.datetime.now()))\n",
    "ss = []\n",
    "kmeans_predict_col={}\n",
    "for k in range_n_clusters:\n",
    "    kmeans = getAlgoForCluster(k)\n",
    "    #kmeans.fit(x_cols)\n",
    "    cluster_labels = getFitPredictForKMean(kmeans,x_cols)\n",
    "    if k not in kmeans_predict_col:\n",
    "        kmeans_predict_col[k]=cluster_labels\n",
    "    #print('k='+str(k))\n",
    "    #print('inertia '+str(kmeans.inertia_))\n",
    "    ss.append(kmeans.inertia_)\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(ss))\n",
    "print(min(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1,1,figsize=(8,8),dpi=100)\n",
    "_ = plt.plot(range_n_clusters, ss, 'ro-', linewidth = 1.0)\n",
    "_ = plt.xlim([1,15])\n",
    "_ = plt.xlabel('K')\n",
    "_ = plt.ylim([714603621,7511630524])\n",
    "_ = plt.ylabel('Sum of Squares(SS)')\n",
    "_ = plt.title('Elbow Method (2-10)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Method for K \n",
    "<pre>\n",
    "\n",
    "| Range       | Interpretation                                |\n",
    "|-------------|-----------------------------------------------|\n",
    "| 0.71 - 1.0  | A strong structure has been found.            |\n",
    "| 0.51 - 0.7  | A reasonable structure has been found.        |\n",
    "| 0.26 - 0.5  | The structure is weak and could be artificial.|\n",
    "| < 0.25      | No substantial structure has been found.      |\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(datetime.datetime.now()))\n",
    "silhouette_avgscores = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "#     cluster_labels = getFitPredictForKMean(getAlgoForCluster(n_clusters),x_cols)\n",
    "    if n_clusters not in kmeans_predict_col:\n",
    "        cluster_labels = getFitPredictForKMean(getAlgoForCluster(n_clusters),x_cols)\n",
    "        kmeans_predict_col[n_clusters]=cluster_labels\n",
    "    else:\n",
    "        cluster_labels =kmeans_predict_col[n_clusters]\n",
    "        \n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    # The sample size is used to avoid memeory error\n",
    "    silhouette_avg = silhouette_score(x_cols, cluster_labels,sample_size=50000)\n",
    "    silhouette_avgscores.append(silhouette_avg)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "# commented for memory issue\n",
    "   # Create a subplot with 1 row and 2 columns\n",
    "\n",
    "#     fig, ax1 = plt.subplots(1, 1)\n",
    "#     _ = fig.set_size_inches(9, 7)\n",
    "\n",
    "#     # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "#     # lie within [-0.1, 1]\n",
    "#     _ = ax1.set_xlim([-0.1, 1])\n",
    "#     # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "#     # plots of individual clusters, to demarcate them clearly.\n",
    "#     _ = ax1.set_ylim([0, len(x_cols) + (n_clusters + 1) * 10])\n",
    "#     # Compute the silhouette scores for each sample\n",
    "#     sample_silhouette_values = silhouette_samples(x_cols, cluster_labels)\n",
    "\n",
    "#     y_lower = 10\n",
    "#     for i in range(n_clusters):\n",
    "#         # Aggregate the silhouette scores for samples belonging to\n",
    "#         # cluster i, and sort them\n",
    "#         ith_cluster_silhouette_values = \\\n",
    "#             sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "#         ith_cluster_silhouette_values.sort()\n",
    "\n",
    "#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#         y_upper = y_lower + size_cluster_i\n",
    "\n",
    "#         #color = cm.spectral(float(i) / n_clusters)\n",
    "#         color = plt.cm.Spectral(float(i) / n_clusters)\n",
    "#         _ = ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                           0, ith_cluster_silhouette_values,\n",
    "#                           facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         _ = ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#     _ = ax1.set_title((\"The Silhouette plot for KMeans clustering with n_clusters = %d\" % n_clusters), fontsize=14, fontweight='bold')\n",
    "#     _ = ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "#     _ = ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "#     # The vertical line for average silhouette score of all the values\n",
    "#     _ = ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "#     _ = ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#     _ = ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw graph for all above silhouette_score\n",
    "drawAverageSilhouetteScore(range_n_clusters,silhouette_avgscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize cluster for range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for noofcluster in range_n_clusters:#[5,6,8,9,11]:#:range_n_clusters:\n",
    "    if noofcluster in kmeans_predict_col:\n",
    "        algo_val=kmeans_predict_col[noofcluster]\n",
    "    else:\n",
    "        algo_val = getFitPredictForKMean(getAlgoForCluster(noofcluster),x_cols)\n",
    "    drawClusterBar(noofcluster,algo_val)\n",
    "    df_PCA=getPCADataFrame(noofcluster,algo_val,x_cols)\n",
    "    drawClusterPlot(df_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawElbowPCAplot(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the best Kmean and display the data frame with cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noofcluster=3\n",
    "if noofcluster in kmeans_predict_col:\n",
    "    algo_val=kmeans_predict_col[noofcluster]\n",
    "else:\n",
    "    algo_val = getFitPredictForKMean(getAlgoForCluster(noofcluster),x_cols)\n",
    "drawClusterBar(noofcluster,algo_val)\n",
    "df_PCA=getPCADataFrame(noofcluster,algo_val,x_cols)\n",
    "drawClusterPlot(df_PCA)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(type(algo_val))\n",
    "print(type(df_pca_matrix['x']))\n",
    "print(type(df_pca_matrix['x'].values))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_PCA.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=df_PCA.groupby('cluster_label').count()\n",
    "fig, axis = plt.subplots(1,1,figsize=(6,6),dpi=100)\n",
    "_ = v.x.plot(kind='bar')\n",
    "_ = plt.ylabel('Number of Points')\n",
    "_ = plt.xlabel('Cluster')\n",
    "_ = plt.title('No of points for Clusters($K$ = '+str(3)+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_num=1\n",
    "df_merge_w_cluster=pd.merge(df_filterdata, (df_PCA[df_PCA.cluster_label==custer_num]), left_on = 'index', right_on = 'index',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merge_w_cluster.head(2)\n",
    "#df_merge_w_cluster[df_merge_w_cluster['cluster_label'].isna()]\n",
    "print(len(df_merge_w_cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_w_cluster.iloc[:,60:75].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_w_cluster.iloc[:,75:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_w_cluster.iloc[:,70:75].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_w_cluster.iloc[:,75:80].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_w_cluster.iloc[:,80:85].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllcolNameforDataframe(df_merge_w_cluster.iloc[:,60:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllColSum(df_merge_w_cluster.iloc[:,60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_num=2\n",
    "df_merge_w_cluster2=pd.merge(df_filterdata, (df_PCA[df_PCA.cluster_label==custer_num]), left_on = 'index', right_on = 'index',how='inner')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getAllColSum(df_merge_w_cluster2.iloc[:,60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareColForDataFrame(df_merge_w_cluster.iloc[:,60:],df_merge_w_cluster2.iloc[:,60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllcolNameforDataframe(df_filterdata.iloc[:,60:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
